{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69efa4d8-a319-4f21-a9f9-cbe5d2dffd47",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3be7ad53-c772-4f0a-a6c8-c39e448c6740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯУД Basic Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18114 entries, 0 to 18113\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   text      18114 non-null  object\n",
      " 1   label     18114 non-null  object\n",
      " 2   category  18114 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 424.7+ KB\n",
      "None\n",
      "\n",
      "ЁЯФв First 5 Rows:\n",
      "                                                text  label category\n",
      "0  ржжрзЗржмржпрж╛ржи тАУ ржЙржкржирзНржпрж╛рж╕ тАУ ржмрж┐ржнрзВрждрж┐ржнрзВрж╖ржг ржмржирзНржжрзНржпрзЛржкрж╛ржзрзНржпрж╛рзЯ рзз...  human   novels\n",
      "1  ржмрж░ржкржХрзНрж╖рзЗрж░ ржирж┐ржмрж╛рж╕ ржХрж▓ржХрж╛рждрж╛, ржЖржЬржЗ ржмрзЗрж▓рж╛ рждрж┐ржиржЯрзЗрж░ рж╕ржоржпрж╝ ржорзЛ...  human   novels\n",
      "2  рж╣ржмрзЗ, ржХрж▓ржХрж╛рждрж╛ ржерзЗржХрзЗ ржмрж░ржкржХрзНрж╖ ржнрж╛рж▓ ржмрж╛ржЬрж┐ ржПржирзЗржЪрзЗред ржПрж╕ржм ржкрж╛...  human   novels\n",
      "3  ржпрждрзАржи рж╣рзЗрж╕рзЗ ржмрж▓рзНрж▓рзЗтАУржХрзЗржоржи, ржмрж╛ржЬрж╛рж░рзЗрж░ ржЦрж╛ржмрж╛рж░ ржХрж┐ржирждрзЗ рж╣ржмрзЗ ...  human   novels\n",
      "4  рж▓ржЬрзНржЬрж╛ ржУ рж╕ржЩрзНржХрзЛржЪ ржХрж╛ржЯрж┐ржпрж╝рзЗ рж╢рж╛рж╢рзБржбрж╝рзАржХрзЗ ржЬрж┐ржЬрзНржЮрзЗрж╕ ржХрж░рж▓тАУржУ...  human   novels\n",
      "\n",
      "ЁЯзн Dataset Shape:\n",
      "Rows: 18114, Columns: 3\n",
      "\n",
      "ЁЯХ│я╕П Missing Values:\n",
      "text        0\n",
      "label       0\n",
      "category    0\n",
      "dtype: int64\n",
      "\n",
      "Percentage of Missing Values:\n",
      "text        0.0\n",
      "label       0.0\n",
      "category    0.0\n",
      "dtype: float64\n",
      "\n",
      "ЁЯзм Duplicated Rows:\n",
      "1\n",
      "\n",
      "ЁЯУж Data Types:\n",
      "text        object\n",
      "label       object\n",
      "category    object\n",
      "dtype: object\n",
      "\n",
      "ЁЯз▒ Constant Columns:\n",
      "[]\n",
      "\n",
      "ЁЯФН Unique Value Count per Column:\n",
      "text        18113\n",
      "label           2\n",
      "category        4\n",
      "dtype: int64\n",
      "\n",
      "ЁЯУК Statistical Summary (Numeric):\n",
      "                                                     text  label category\n",
      "count                                               18114  18114    18114\n",
      "unique                                              18113      2        4\n",
      "top     ржХрж┐ржирзНрждрзБ рждрж╛ржБрж░рж╛ ржХрзЗржЙ ржмрзНржпрж╛ржХрж░ржгрж░ржЪржпрж╝рж┐рждрж╛ ржЫрж┐рж▓рзЗржи ржирж╛;- ржПржХ ...  human   novels\n",
      "freq                                                    2   9057     5000\n",
      "\n",
      "ЁЯЧВя╕П Summary (Categorical):\n",
      "                                                     text  label category\n",
      "count                                               18114  18114    18114\n",
      "unique                                              18113      2        4\n",
      "top     ржХрж┐ржирзНрждрзБ рждрж╛ржБрж░рж╛ ржХрзЗржЙ ржмрзНржпрж╛ржХрж░ржгрж░ржЪржпрж╝рж┐рждрж╛ ржЫрж┐рж▓рзЗржи ржирж╛;- ржПржХ ...  human   novels\n",
      "freq                                                    2   9057     5000\n",
      "\n",
      "ЁЯз╛ Top Value Counts (Categorical Columns):\n",
      "\n",
      "text:\n",
      "text\n",
      "ржХрж┐ржирзНрждрзБ рждрж╛ржБрж░рж╛ ржХрзЗржЙ ржмрзНржпрж╛ржХрж░ржгрж░ржЪржпрж╝рж┐рждрж╛ ржЫрж┐рж▓рзЗржи ржирж╛;- ржПржХ ржжрж╢ржХрзЗрж░ ржорждрзЛ рж╕ржоржпрж╝ ржзтАЩрж░рзЗ рждрж╛ржБрж░рж╛ ржмрж╛ржЩрж▓рж╛ ржнрж╛рж╖рж╛ ржмрж░рзНржгржирж╛рж░ ржЬржирзНржпрзЗ ржирждрзБржи ржХрж╛ржарж╛ржорзЛ ржЦрзБржБржЬрзЗ ржпрзЗ-ржпрж╛ржБрж░ ржирж┐ржЬ ржПрж▓рж╛ржХрж╛ржпрж╝ ржлрж┐рж░рзЗ ржпрж╛ржи, ржПржмржВ ржкрзНрж░ржерж╛ржЧржд ржмрж╛ржЩрж▓рж╛ ржмрзНржпрж╛ржХрж░ржгрзЗрж░ ржзрж╛рж░рж╛ ржмржЗрждрзЗ ржерж╛ржХрзЗ ржкрзНрж░ржерж╛ржЧржд ржЦрж╛рждрзЗржЗред ржмрж┐рж╢рж╢рждржХрзЗ рж░ржЪрж┐ржд ржмрж╛ржЩрж▓рж╛ ржмрзНржпрж╛ржХрж░ржг ржкрзБрж╕рзНрждржХржЧрзБрж▓рзЛ ржкрзНрж░ржХрзГрждрж┐ржкрзНрж░ржгрж╛рж▓рж┐рждрзЗ ржЙржирж┐рж╢рж╢рждржХрж┐ ржмрзНржпрж╛ржХрж░ржгржЗ, ржпржжрж┐ржУ ржХрзЛржирзЛржХрзЛржирзЛржЯрж┐ ржмрзЗрж╢ ржмрзНржпрж╛ржкржХред ржмрж╛ржЩрж▓рж╛ ржмрзНржпрж╛ржХрж░ржгржХрж╛ржарж╛ржорзЛ ржУ тАШржиржмржмрзНржпрж╛ржХрж░ржгржмрж┐ржжтАЩржЧржгред ржЙржирж┐рж╢рж╢рждржХрзЗрж░ рж╢рзЗрж╖ ржжрж╢ржХрзЗрж░ рж╢рзЗрж╖рж╛ржВрж╢ ржкрж░рзНржпржирзНржд ржмрж╛ржЩрж▓рж╛ ржнрж╛рж╖рж╛рждрждрзНрждрзНржм рж╕рзАржорж╛ржмржжрзНржз ржерж╛ржХрзЗ рж╕рж╛ржзрж╛рж░ржгржд ржЕржнрж┐ржзрж╛ржирзЗ ржУ ржкрзНрж░ржерж╛ржЧржд ржмрзНржпрж╛ржХрж░ржгрзЗред ржорж╛ржЭрзЗржорж╛ржЭрзЗ ржмрж╛ржЩрж▓рж╛ рж╢ржмрзНржжрж╕ржорзНржнрж╛рж░, ржмрж╛ржЩрж▓рж╛ ржнрж╛рж╖рж╛рж░ ржЪрж╛рж░рж┐рждрзНрж░, ржмрж░рзНржгржорж╛рж▓рж╛ ржкрзНрж░ржнрзГрждрж┐ рж╕ржорзНржкрж░рзНржХрзЗ ржкрзНрж░ржХрж╛рж╢рж┐ржд рж╣ржпрж╝ ржХрж┐ржЫрзБ ржмрж┐ржЪрзНржЫрж┐ржирзНржи ржкрзНрж░ржмржирзНржз (ржжрзНрж░ ржмржЩрзНржХрж┐ржоржЪржирзНржжрзНрж░ (рззрзирзорзл), ржЧрзНрж░рж╛ржбрзБржПржЯ (рззрзирзорзо), рж░рж╛ржЬрзЗржирзНржжрзНрж░рж▓рж╛рж▓ (рззрзорзмрзм, рззрзорзнрзн), рж╢рзНржпрж╛ржорж╛ржЪрж░ржг (рззрзорзнрзн))ред рждржЦржирзЛ ржЕржирзЗржХрзЗржЗ ржорзЗржирзЗ ржирж┐рждрзЗ ржкрж╛рж░рзЗржи ржирж┐ ржпрзЗ ржмрж╛ржЩрж▓рж╛ (тАШржмрж╛ржЩрзНржЧрж╛рж▓рж╛тАЩ, ржмрж╛ тАШржмрж╛ржЩрзНржЧрж▓рж╛тАЩ) ржПржХржЯрж┐ рж╕рзНржмрждржирзНрждрзНрж░ рж╕рзНржмрж╛ржзрзАржи рж╕рзНржмрж╛ржпрж╝рждрзНрждрж╢рж╛рж╕рж┐ржд ржнрж╛рж╖рж╛; ржПржмржВ рж╕рзНржмрждржирзНрждрзНрз░ рж╕ржВрж╕рзНржХрзГржд ржерзЗржХрзЗ; рждрж╛ржЗ рждрж╛рж░ ржмрзНржпрж╛ржХрж░ржгржХрж╛ржарж╛ржорзЛржУ ржЕржирж┐ржмрж╛рж░рзНржпржнрж╛ржмрзЗ рж╕рзНржмрждржирзНрждрзНрж░ рж╣ржмрзЗ рж╕ржВрж╕рзНржХрзГржд ржмрзНржпрж╛ржХрж░ржгржХрж╛ржарж╛ржорзЛ ржерзЗржХрзЗред ржмрж╛ржЩрж▓рж╛ ржнрж╛рж╖рж╛рж░ рж╕рзНржмрж╛рждржирзНрждрзНрж░рзНржп ржУ рж╕рзНржмрж╛ржпрж╝рждрзНрждрж╢рж╛рж╕ржи, ржПржмржВ ржмрж╛ржЩрж▓рж╛ ржмрзНржпрж╛ржХрж░ржгржХрж╛ржарж╛ржорзЛрж░ рж╕рзНржмрж╛ржзрж┐ржХрж╛рж░ ржкрзНрж░ржержо, ржУ ржЕржирзЗржХржЯрж╛ ржкрж░рзЛржХрзНрж╖ржнрж╛ржмрзЗ, ржжрж╛ржмрж┐ ржХрж░рж╛ рж╣ржпрж╝ ржмрж╛рж▓ржХ, ржУ рж╕рж╛ржзржирж╛ ржкрждрзНрж░рж┐ржХрж╛ржпрж╝ ржкрзНрж░ржХрж╛рж╢рж┐ржд рждрж░рзБржг рж░ржмрзАржирзНржжрзНрж░ржирж╛ржерзЗрж░ ржХржпрж╝рзЗржХржЯрж┐ ржкрзНрж░ржмржирзНржзрзЗ- тАШржмрж╛ржВрж▓рж╛ ржЙржЪрзНржЪрж╛рж░ржгтАЩ (рззрзирзпрзи), тАШрж╕рзНржмрж░ржмрж░рзНржг ржЕтАЩ (рззрзирзпрзп), тАШрж╕рзНржмрж░ржмрж░рзНржг ржПтАЩ (рззрзирзпрзп), ржУ тАШржЯрж╛ ржЯрзЛ ржЯрзЗтАЩ (рззрзирзпрзп)-рждрзЗред рззрзйрзжрзз-ржП (рззрзорзпрзк) ржкрзНрж░ржХрж╛рж╢рж┐ржд рж╣ржпрж╝ тАЬрж╕рж╛рж╣рж┐рждрзНржп-ржкрж░рж┐рж╖рзО-ржкрждрзНрж░рж┐ржХрж╛тАЭ, ржпрж╛рж░ ржЪрждрзБрж░рзНрже ржУ ржкржЮрзНржЪржо ржмрж░рзНрж╖рзЗрж░ ржжрзБ-рж╕ржВржЦрзНржпрж╛ржпрж╝ ржкрзНрж░ржХрж╛рж╢рж┐ржд рж╣ржпрж╝ ржжрзНржмрж┐ржЬрзЗржирзНржжрзНрж░ржирж╛рже ржарж╛ржХрзБрж░рзЗрж░ тАШржЙржкрж╕рж░рзНржЧрзЗрж░ ржЕрж░рзНржержмрж┐ржЪрж╛рж░тАЩ (рззрзйрзжрзк, рззрзйрзжрзл)ред тАШржЙржкрж╕рж░рзНржЧрзЗрж░ ржЕрж░рзНржержмрж┐ржЪрж╛рж░тАЩ ржирж╛ржоржХ ржжрзАрж░рзНржШ ржУ ржЕржирзНрждрж░рзНржжрзГрж╖рзНржЯрж┐рж╕ржорзНржкржирзНржи ржкрзНрж░ржмржирзНржзржЯрж┐рждрзЗржЗ рж╕рзБрж╕рзНржкрж╖рзНржЯржнрж╛ржмрзЗ ржмрж┐ржХрж╢рж┐ржд рж╣ржпрж╝ ржиржмржмрзНржпрж╛ржХрж░ржг- ржжрзГрж╖рзНржЯрж┐, ржпрж╛рж░ рж▓ржХрзНрж╖рзНржп ржмрж╛ржЩрж▓рж╛ ржнрж╛рж╖рж╛ржХрзЗ ржмрж╛ржЩрж▓рж╛ ржнрж╛рж╖рж╛ рж╣рж┐рж╢рзЗржмрзЗржЗ ржмрзНржпрж╛ржЦрзНржпрж╛ржмрж┐рж╢рзНрж▓рзЗрж╖ржг ржХрж░рж╛ред ржкрзНрж░ржмржирзНржзржЯрж┐ рж╕ржВрж╕рзНржХрзГрждржкржирзНржерзАржжрзЗрж░ ржЖрждржЩрзНржХрж┐ржд, ржУ ржиржмрзНржпржкржирзНржерзАржжрзЗрж░ ржЕржирзБржкрзНрж░рж╛ржгрж┐ржд ржХрж░рзЗржЫрж┐рж▓рзЛред ржжрзНржмрж┐ржЬрзЗржирзНржжрзНрж░ржирж╛ржерзЗрж░ ржкрзНрж░ржмржирзНржзрзЗрж░ ржкрзНрж░рждрж┐ржмрж╛ржжрзЗ рж░рж╛ржЬрзЗржирзНржжрзНрж░ржЪржирзНржжрзНрж░ рж╢рж╛рж╕рзНрждрзНрж░рзА рж░ржЪржирж╛ ржХрж░рзЗржи тАШржЙржкрж╕рж░рзНржЧрзЗрж░ ржЕрж░рзНржержмрж┐ржЪрж╛рж░ ржирж╛ржоржХ ржкрзНрж░ржмржирзНржзрзЗрж░ рж╕ржорж╛рж▓рзЛржЪржирж╛тАЩ (рззрзйрзжрзл), ржПржмржВ рждрж╛ржБрж░ ржкрзНрж░рждрж┐ржмрж╛ржжрзЗ рж░ржмрзАржирзНржжрзНрж░ржирж╛рже рж▓рж┐ржЦрзЗржи тАШржЙржкрж╕рж░рзНржЧ-рж╕ржорж╛рж▓рзЛржЪржирж╛тАЩ (рззрзйрзжрзм)ред ржП-ржкрзНрж░ржмржирзНржз ржХрзЗржирзНржжрзНрж░ ржХтАЩрж░рзЗржЗ ржмрж╛ржЩрж▓рж╛ ржнрж╛рж╖рж╛ржмрж┐ржж рж╕ржорзНржкрзНрж░ржжрж╛ржпрж╝ ржмрж┐ржнржХрзНржд рж╣тАЩржпрж╝рзЗ ржпрж╛ржи ржжрзБ-ржжрж▓рзЗ : ржПржХржжрж▓ржХрзЗ ржмрж▓рждрзЗ ржкрж╛рж░рж┐ тАШржиржмржмрзНржпрж╛ржХрж░ржгржмрж┐ржжтАЩ ржмрж╛ ржмрж╛ржЩрж▓рж╛ржкржирзНржерзА, ржУ ржЕржирзНржпржжрж▓ржХрзЗ ржмрж▓рждрзЗ ржкрж╛рж░рж┐ тАШржкрзБрж░рзЛржирзЛ ржмрзНржпрж╛ржХрж░ржгржмрж┐ржжтАЩ ржмрж╛ рж╕ржВрж╕рзНржХрзГрждржкржирзНржерзАред ржжрзНржмрж┐ржЬрзЗржирзНржжрзНрж░ржирж╛рже ржпрзЗ-ржЖржирзНржжрзЛрж▓ржирзЗрж░ рж╕рзВржЪржирж╛ ржХрж░рзЗржи, рждрж╛ржХрзЗ ржПржЧрж┐ржпрж╝рзЗ ржирж┐ржпрж╝рзЗ ржпрж╛ржи рж░ржмрзАржирзНржжрзНрж░ржирж╛рже, рж░рж╛ржорзЗржирзНржжрзНрж░рж╕рзБржирзНржжрж░ рждрзНрж░рж┐ржмрзЗржжрзА, рж╣рж░ржкрзНрж░рж╕рж╛ржж рж╢рж╛рж╕рзНрждрзНрж░рзАред рждрж╛ржБржжрзЗрж░ ржмрж┐рж░рзЛржзрзАржЧрзЛрждрзНрж░рзЗ рж╕ржХрзНрж░рж┐ржпрж╝ржнрж╛ржмрзЗ ржЕржмрж╕рзНржерж╛ржи ржХрж░рзЗржи рж╢рж░ржЪрзНржЪржирзНржжрзНрж░ рж╢рж╛рж╕рзНрждрзНрж░рзА, рж╕рждрзАрж╢ржЪржирзНржжрзНрж░ ржмрж┐ржжрзНржпрж╛ржнрзВрж╖ржг, рж╢рзНрз░рзАржирж╛рже рж╕рзЗржи ржкрзНрж░ржорзБржЦред рж░ржмрзАржирзНржжрзНрж░ржирж╛ржерзЗрж░ тАШржмрж╛ржВрж▓рж╛ рж╢ржмрзНржжржжрзНржмрзИрждтАЩ (рззрзйрзжрзн), тАШржзрзНржмржирзНржпрж╛рждрзНржоржХ рж╢ржмрзНржжтАЩ (рззрзйрзжрзо), тАШржмрж╛ржВрж▓рж╛ ржХрзГрзО ржУ рждржжрзНржзрж┐рждтАЩ (рззрзйрзжрзо) ржкрзНрж░рждрж┐рж╖рзНржарж┐ржд ржХрж░рзЗ рждрж╛ржБржХрзЗ ржиржмржмрзНржпрж╛ржХрж░ржгржмрж┐ржжрж╕ржорзНржкрзНрж░ржжрж╛ржпрж╝рзЗрж░ ржкрзБрж░рзЛржзрж╛рж░рзВржкрзЗред ржП-ржЖржирзНржжрзЛрж▓ржиржХрзЗ рждрзАржмрзНрж░-ржмрзНржпрж╛ржкржХ ржХтАЩрж░рзЗ рждрзЛрж▓рзЗ рж╣рж░ржкрзНрж░рж╕рж╛ржж рж╢рж╛рж╕рзНрждрзНрж░рзАрж░ тАШржмрж╛ржЩрзНржЧрж╛рж▓рж╛ ржмрзНржпрж╛ржХрж░ржгтАЩ (рззрзйрзжрзо) ржУ рж░рж╛ржорзЗржирзНржжрзНрж░рж╕рзБржирзНржжрж░ рждрзНрж░рж┐ржмрзЗржжрзАрж░ тАШржмрж╛ржЩрзНржЧрж▓рж╛ ржмрзНржпрж╛ржХрж░ржгтАЩ (рззрзйрзжрзо), тАШржмрж╛ржЩрзНржЧрж╛рж▓рж╛ ржХрж╛рж░ржХ-ржкрзНрж░ржХрж░ржгтАЩ (рззрзйрззрзи), ржУ тАШржзрзНржмржирж┐-ржмрж┐ржЪрж╛рж░тАЩ (рззрзйрззрзк) ржкрзНрж░ржмржирзНржзред ржП-ржЖржирзНржжрзЛрж▓ржиржХрзЗ ржкрзНрж░рждрж┐рж░рзЛржзрзЗрж░ ржЪрзЗрж╖рзНржЯрж╛ ржХрж░рж╛ рж╣ржпрж╝ рж╢рж░ржЪрзНржЪржирзНржжрзНрж░ рж╢рж╛рж╕рзНрждрзНрж░рзАрж░ тАШржирзВрждржи ржмрж╛ржЩрзНржЧрж╛рж▓рж╛ ржмрзНржпрж╛ржХрж░ржгтАЩ (рззрзйрзжрзо), тАШржмрзНржпрж╛ржХрж░ржг ржУ ржмрж╛ржЩрзНржЧрж╛рж▓рж╛ ржнрж╛рж╖рж╛тАЩ (рззрзйрзжрзо), рж╕рждрзАрж╢ржЪржирзНржжрзНрж░ ржмрж┐ржжрзНржпрж╛ржнрзВрж╖ржгрзЗрж░ тАШржнрж╛рж╖рж╛рж░ рж╕рж╣рж┐ржд ржмрзНржпрж╛ржХрж░ржгрзЗрж░ рж╕ржорзНржмржирзНржзтАЩ (рззрзйрзжрзо), тАШржмрж╛ржЩрзНржЧрж╛рж▓рж╛ ржнрж╛рж╖рж╛рж░ ржмрзНржпрж╛ржХрж░ржгтАЩ (рззрзйрззрзз), рж╢рзНрж░рзАржирж╛рже рж╕рзЗржирзЗрж░ тАШржкрзНрж░рж╛ржХрзГржд ржмрзНржпрж╛ржХрж░ржг ржУ ржЕржнрж┐ржзрж╛ржитАЩ (рззрзйрззрзм) ржкрзНрж░ржмржирзНржзрзЗред ржиржмржмрзНржпрж╛ржХрж░ржгржмрж┐ржжржжрзЗрж░ ржорзБржЦржкрждрзНрж░ ржЫрж┐рж▓рзЛ рж╕рж╛рж╣рж┐рждрзНржп-ржкрж░рж┐рж╖рзО-ржкрждрзНрж░рж┐ржХрж╛, рж╕ржВрж╕рзНржХрзГрждржкржирзНржерзАржжрзЗрж░ ржорзБржЦржкрждрзНрж░ ржЫрж┐рж▓рзЛ ржкрзНрж░ржзрж╛ржиржд ржнрж╛рж░рждрзАред ржиржмрзНржп ржУ ржкрзБрж░рзЛржирзЛржкржирзНржерзАржжрзЗрж░ ржмрж┐рждрж░рзНржХрзЗрж░ ржЫрж┐рж▓рзЛ ржжрзБржЯрж┐ ржЕржмрж┐ржЪрзНржЫрзЗржжрзНржп ржмрж┐рж╖ржпрж╝- ржмрж╛ржЩрж▓рж╛ ржнрж╛рж╖рж╛ ржУ ржмрзНржпрж╛ржХрж░ржгржХрж╛ржарж╛ржорзЛрж░ рж╕рзНржмрж╛ржзрж┐ржХрж╛рж░ ржмржирж╛ржо рж╕ржВрж╕рзНржХрзГржд-ржЕржзрзАржирждрж╛ред ржиржмржмрзНржпрж╛ржХрж░ржгржмрж┐ржж ржЕрж░рзНржерж╛рзО ржмрж╛ржЩрж▓рж╛ржкржирзНржерзАрж░рж╛ ржирж┐рж╢рзНржЪрж┐ржд ржзрж░рзЗ ржирж┐ржпрж╝рзЗржЫрж┐рж▓рзЗржи ржпрзЗ ржмрж╛ржЩрж▓рж╛ ржПржХржЯрж┐ рж╕рзНржмрждржирзНрждрзНрж░ рж╕рзНржмрж╛ржпрж╝рждрзНрждрж╢рж╛рж╕рж┐ржд ржнрж╛рж╖рж╛;- рж╕ржВрж╕рзНржХрзГрждрзЗрж░ рж╕рж╛ржерзЗ рждрж╛рж░ рж╕ржорзНржкрж░рзНржХ рж╕рзБржжрзВрж░ред рждржЦржиржХрж╛рж░ ржкрзНрж░ржерж╛ржЧржд ржзрж╛рж░ржгрж╛- ржмрж╛ржЩрж▓рж╛ рж╕ржВрж╕рзНржХрзГрждрзЗрж░ ржХржирзНржпрж╛тАУрждрж╛ржБржжрзЗрж░ ржХрж╛ржЫрзЗ ржЧрзНрж░рж╣ржгржпрзЛржЧрзНржп ржЫрж┐рж▓рзЛ ржирж╛, ржмрж░ржВ рждрж╛ржБрж░рж╛ ржмрж┐рж╢рзНржмрж╛рж╕рзА ржЫрж┐рж▓рзЗржи ржкрзНрж░рж╛ржХрзГрждрзЗрж░ рж╕рж╛ржерзЗржЗ ржмрж╛ржЩрж▓рж╛ ржнрж╛рж╖рж╛рж░ ржШржирж┐рж╖рзНржа рж╕ржорзНржкрж░рзНржХрзЗред рждрж╛ржБрж░рж╛ ржжрзЗржЦрзЗржЫрзЗржи ржзрзНржмржирж┐рждрзЗ, рж╢ржмрзНржжрзЗ, ржмрж╛ржХрзНржпрзЗ, ржЕрж░рзНржерзЗ рж╕ржВрж╕рзНржХрзГрждрзЗрж░ рж╕рж╛ржерзЗ ржжрзБрж╕рзНрждрж░ ржмрзНржпржмржзрж╛ржи ржмрж╛ржЩрж▓рж╛рж░ред ржпржжрж┐ржУ ржмрж╛ржЩрж▓рж╛ ржнрж╛рж╖рж╛ ржЖрждрзНржорж╕рзНрже ржХтАЩрж░рзЗржЫрзЗ ржмрж┐ржкрзБрж▓ ржкрж░рж┐ржорж╛ржг рж╕ржВрж╕рзНржХрзГржд рж╢ржмрзНржж, рждржмрзБржУ ржмрж╛ржЩрж▓рж╛ рж╣тАЩржпрж╝рзЗ ржЙржарзЗржЫрзЗ ржПржХржЯрж┐ ржкрзГржержХ ржнрж╛рж╖рж╛, ржпрж╛рж░ ржмрзНржпрж╛ржХрж░ржг рж╣ржмрзЗ ржкрзГржержХред    2\n",
      "ржжрзЗржмржпрж╛ржи тАУ ржЙржкржирзНржпрж╛рж╕ тАУ ржмрж┐ржнрзВрждрж┐ржнрзВрж╖ржг ржмржирзНржжрзНржпрзЛржкрж╛ржзрзНржпрж╛рзЯ рзз. рж╕рз░рзНржмрзНржмрж╛ржЬрзАржмрзЗ рж╕рз░рзНржмрзНржмрж╕ржВрж╕рзНржерзЗ ржмрзГрж╣ржирзНрждрзЗред ржЕрж╕рзНржорж┐ржирзН рж╣ржВрж╕рзЛ ржнрзНрж░рж╛ржорзНржпрждрзЗ ржмрзНрж░рж╣рзНржоржЪржХрзНрж░рзЗ тАУрж╢рзНржмрзЗрждрж╛рж╢рзНржмрждрж░ ржЙржкржирж┐рж╖рзО рзи. ржи ржЬрж╛ржпрж╝рждрзЗ ржорзНрж░рж┐ржпрж╝рждрзЗ ржмрж╛ ржХржжрж╛ржЪрж┐ ржирзНржирж╛ржпрж╝ржВ ржнрзВрждрзНржмрж╛ ржнржмрж┐рждрж╛ ржмрж╛ ржи ржнрзВржпрж╝ржГред ржЕржЬрзЛ ржирж┐рждрзНржпржГ рж╢рж╛рж╢рзНржмрждрзЛржЗржпрж╝ржВ ржкрзБрж░рж╛ржгрзЛ ржи рж╣ржирзНржпрждрзЗ рж╣ржирзНржпржорж╛ржирзЗ рж╢рж░рзАрж░рзЗредред тАУржнржЧржмржжржЧрзАрждрж╛ рзй. But Mind, Life and Matter, the lower trilogy, are also indispensable to all cosmic beings not necessarily in the form or with the action and conditions which we know upon earth or in this material universe, but in some kind of action however luminous, how ever puissant, however subtle. For Mind is essentialy that faculty of super-mind which measures and limits, which fixes a particular centre and views from that the cosmic movement. SRI AUROBINDOтАЩS The Life Divine, Vol. I. рзк. Beyond these subtle physical planes of experience and the life-worlds there are also mental planes to which the soul seems to have an internatal accessтАж but it is not likely to live consciously there if there has not been a sufficient mental or soul development in this lifeтАж рзл. We know that he creates images of these superior planes which are often mental translation of certain elements in them and erects his images into a system, a form in actual worlds; he builds up also desireworlds of many kinds to which he attaches a strong sense of inner reality. Vol. III, p. 77. рзм. We arrive then necessarily at this conclusion that human birth is a term at which the soul must arrive in a long succession of rebirths and that it has had for its previous and preparatory terms in the succession the lower forms of life upon earthтАж рзн. тАЬGod is Love and object of Love. Divine Love is not a thing of God: it is God Himself. God needs us just as we need God. This universe is the mere Visible tangi ble aspect of Love and of the need of love.тАЭ HENRI BERGSON. . рзжрзз. ржХрзБрзЬрзБрж▓рзЗ-ржмрж┐ржирзЛржжржкрзБрж░рзЗрж░ ржмрж┐ржЦрзНржпрж╛ржд ржмрж╕рзНрждрзНрж░ржмрзНржпржмрж╕рж╛ржпрж╝рзА рж░рж╛ржпрж╝рж╕рж╛рж╣рзЗржм ржнрж░рж╕рж╛рж░рж╛ржо ржХрзБржирзНржбрзБрж░ ржПржХржорж╛рждрзНрж░ ржХржирзНржпрж╛рж░ ржЖржЬ ржмрж┐ржмрж╛рж╣ред                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     1\n",
      "ржорж╛ржЭрзЗрж░ ржЕржВрж╢\\n\\nржорж╛ржЭрзЗ ржоржзрзНржпрзЗ ржмрж░рзЗржи ржмрзБржЭрждрзЗ ржкрж╛рж░рж▓, ржЖржЬржХрзЗрж░ ржорж┐рж╢ржи ржпрждржЗ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг рж╣рзЛржХ, ржПржЦрж╛ржирзЗ ржирж┐ржпрж╝ржирзНрждрзНрж░ржг ржирж╛ржорзЗрж░ ржХрзЛржирзЛ ржЬрж┐ржирж┐рж╕ ржирзЗржЗред ржПржХ ржжрж┐ржХрзЗ ржжрж╛ржБржбрж╝рж┐ржпрж╝рзЗ ржЫрж┐рж▓ ржмрж┐рж╢рж╛рж▓ ржЖржХрзГрждрж┐рж░ рж╢рзВржХрж░ ржорзЗрж╣рзЗржжрж┐, ржпрж╛ржХрзЗ ржХрж┐ржирждрзЗ рж╕рзЗ ржЗрждрж┐ржоржзрзНржпрзЗржЗ рждрзЗрж▓рждрзЗрж▓рзЗ ржмрж╛ржЩрж╛рж▓рж┐ ржмрзБржжрзНржзрж┐ ржЦрж╛ржЯрж╛ржЪрзНржЫрж┐рж▓ред ржарж┐ржХ рждржЦржи ржорзЗрж╣рзЗржжрж┐ рж╣ржЯрзНржЯржЧрзЛрж▓ ржХрж░рждрзЗ ржХрж░рждрзЗржЗ рж▓рзЗржЬ ржжрзБрж▓рж┐ржпрж╝рзЗ ржжрзМржбрж╝рж▓ ржкрж╛рж╢рзЗрж░ ржнрж╛ржЩрж╛ржЪрзЛрж░рж╛ рж╣рж╛ржБржбрж╝рж┐рж░ ржжрж┐ржХрзЗред рж╣рж╛ржБржбрж╝рж┐ ржлрзЗржЯрзЗ ржпрж╛ржУржпрж╝рж╛ ржорж╛рждрзНрж░ ржнрзЗрждрж░ ржерзЗржХрзЗ ржмрзЗрж░рж┐ржпрж╝рзЗ ржПрж▓ ржПржХ ржнрж╛рж╕ржорж╛ржи ржкрж╛ржБржарж╛ ржорж╛ржЫрзЗрж░ ржЦ skeletonтАФржХрж▓ржЩрзНржХрж┐ржд ржПржмржВ ржЫрзЛржЯрзНржЯ, ржХрж┐ржирзНрждрзБ рж╕рж╛рж╣рж╕рзА ржжрзЗржЦрж╛ржпрж╝ ржПржоржи, ржпрзЗржи ржмрж▓ржЫрзЗ, тАЬржЖржорж┐ рж░рж╛ржЬржХрзБржорж╛рж░, ржЕржнрж┐рж╢ржкрзНржд!тАЭ\\n\\nржмрж╛ржЬрж╛рж░рзЗрж░ рж▓рзЛржХржЬржи ржЪржоржХрзЗ ржЪрж┐рзОржХрж╛рж░ ржХрж░рждрзЗ рж▓рж╛ржЧрж▓: тАЬрж░рж╛ржЬржХрзБржорж╛рж░! рж░рж╛ржЬржХрзБржорж╛рж░!тАЭ ржХрзЗржЙ ржХрзЗржЙ ржжрзЛржХрж╛ржирзЗрж░ ржЫрж╛рждрж╛ ржЙрж▓ржЯрзЗ ржжрж┐рж▓, ржХрзЗржЙ ржХрзЗржЙ ржЫрзБржЯрждрзЗ рж▓рж╛ржЧрж▓ ржЖрж▓рзБ-ржЭрзЛрж▓рзЗрж░ ржХрж╛ржБржХржбрж╝рж╛ ржмрж┐ржХрзНрж░рзЗрждрж╛рж░ ржжрж┐ржХрзЗтАФржР рждрзЛ ржнрж╛ржЩрзНржЧрж╛ рж╣рж╛ржБржбрж╝рж┐ ржЖрж░ ржорж╛ржЫрзЗрж░ ржЦ skeleton ржирж┐ржпрж╝рзЗ ржХрж░рзНржиржлрж╛рж░рзНржб ржЦрж╛ржУржпрж╝рж╛ржирзЛрж░ рждрзЛ ржХрзЛржирзЛ ржЙржкрж╛ржпрж╝ ржирзЗржЗ! ржмрж░рзЗржи ржирж┐ржЬрзЗ ржкрж░рзНржпржирзНржд ржирж╛ ржмрзБржЭрзЗржЗ ржлрзЗрж▓рж▓, рж╕рзЗ ржПржЦржи ржХрзА ржХрж░ржЫрзЗ: ржорж╛ржЫрзЗрж░ ржЦ skeleton ржХрзЗ ржпрждржХрзНрж╖ржг ржзрж░рзЗ ржмрзЛржЭрж╛ржмрзЛ рж╕рзЗ ржЖрж░ рж░рж╛ржЬржХрзБржорж╛рж░ ржиржпрж╝, рждрждржХрзНрж╖ржг ржкрж░рзНржпржирзНржд ржмрж╛ржЬрж╛рж░ рж╕ржмрж╛рж░ ржжрж╛ржБрждрзЗрж░ ржорж╛ржЬрж╛ ржлрж╛ржБржХрж╛ рж╣ржпрж╝рзЗ ржпрж╛ржмрзЗред\\n\\nржмрж┐рж╖ржпрж╝ ржЖрж░ржУ ржЬржЯрж┐рж▓ рж╣ржпрж╝ ржпржЦржи ржкрж╛рж╢рзЗрж░ рж╕рзНржЯрж▓рзЗ ржмрж╕рж╛ рж╕рзБрж░рзЗржи ржЪрж┐рзОржХрж╛рж░ ржХрж░рзЗ ржЙржарж▓, тАЬрж▓рж╛рж▓ ржорзЛрж░ржЧрзЗрж░ рж╕ржЩрзНржЧрзЗ рж╢рж╛ржорзБржХрзЗрж░ ржбрзНржпрж╛ржирзНрж╕ рж╢рзЛ рж╢рзБрж░рзБ рж╣ржЪрзНржЫрзЗтАФржПржХ ржжрзЗржЦрждрзЗ ржЖрж╕рзБржи!тАЭ ржмрж░рзЗржи ржШрж╛ржбрж╝ ржШрзБрж░рж┐ржпрж╝рзЗ рждрж╛ржХрж┐ржпрж╝рзЗ ржжрзЗржЦрж▓, рж▓рж╛рж▓ рж░ржЩрзЗрж░ ржкрж╛ржЦржирж╛ ржкрзЬрзЗ ржПржХ ржмрж╛рж▓рждрж┐ ржнрж╛ржЩрзНржЧрж╛ рж╢рж╛ржорзБржХ ржоржЮрзНржЪрзЗ рж╣рзЗржБржЯрзЗ ржЪрж▓рзЗржЫрзЗ, рж╕рж╛ржоржирзЗ ржмрж╕рзЗ ржЖржЫрзЗ ржЧрж┐ржЯрж╛рж░ рж╣рж╛рждрзЗ ржжрзБржЗ ржЬржи ржнржЧрзНржи ржжрзЛржХрж╛ржирж┐, ржЖрж░ ржорж╛ржЗржХрзЗ ржШрзЛрж╖ржгрж╛ ржХрж░рж╛ рж╣ржЪрзНржЫрзЗ, тАЬржПржЗ рж╢рж╛ржорзБржХ ржжрзЗржЦрзЗ ржкрзНрж░рждрж┐ржпрзЛржЧрзА ржиржорзНржмрж╛рж░ рждрж┐ржирзЗрж░ ржЪрзЛржЦ ржерзЗржХрзЗржЗ ржкрж╛ржирж┐ ржЭрж░ржмрзЗ!тАЭ\\n\\nржПржжрж┐ржХрзЗ рж╣ржЯрзНржЯржЧрзЛрж▓рзЗрж░ рж╢ржмрзНржжрзЗ ржЧрзНрж░рж╛ржорзЗрж░ ржкржЮрзНржЪрж╛ржпрж╝рзЗрждрзЗрж░ ржмрж╕ржирзНрждрж┐ ржмржЙ ржЫрзБржЯрзЗ ржПрж▓рзЛ, ржкрзЗржЫржирзЗ ржЬржбрж╝рж┐ржпрж╝рзЗ ржирж┐ржпрж╝рзЗржЫрж┐рж▓ ржПржХ ржмрж┐рж╢рж╛рж▓ ржкрж╛ржЯрзЗрж░ ржЭрзБржбрж╝рж┐ред ржЭрзБржбрж╝рж┐рж░ ржнрзЗрждрж░ ржерзЗржХрзЗ ржПржХрзЗ ржПржХрзЗ ржмрзЗрж░рзЛржЪрзНржЫрзЗ ржкрзБрж░ржирзЛ ржкрзНрж░рзЗржоржкрждрзНрж░, ржлрзБржЯржмрж▓, рж╣рж╛рждрзЗрж░ ржЖржБржХрж╛ ржирж╛ржЯржХрзАржпрж╝ ржжрзГрж╢рзНржпрзЗрж░ ржлрзЛржЯрзЛржХржкрж┐тАФ ржкрзНрж░рждрж┐ржЯрж╛ ржЬрж┐ржирж┐рж╕ржЗ ржпрзЗржи ржмрж╛ржЬрж╛рж░рзЗ ржЖржЧрзБржи ржзрж░рж┐ржпрж╝рзЗ ржжрж┐ржЪрзНржЫрзЗред ржмрж░рзЗржи ржжрзМржбрж╝рзЗ ржПрж╕рзЗ ржЭрзБржбрж╝рж┐рж░ ржжрж┐ржХрзЗ ржЖржЧрзНрж░рж╣ ржорж┐рж╢рж┐ржпрж╝рзЗ рж╣рзЗрж╕рзЗ ржЙржарж▓: тАЬржмрж╕ржирзНрждрж┐ ржЖржкрж╛, ржПржЗ рждрзЛ ржорж╛ржЭрж╛ржорж╛ржЭрж┐ ржХрж╛ржгрзНржб! рж╢рзЗрж╖рзЗрж░ ржЬржирзНржп ржХрж┐ ржЖрж░ ржХрзЛржирзЛ ржЯрзБржЗрж╕рзНржЯ рж░рж╛ржЦрж▓рзЗржи?тАЭ\\n\\nржмрж╕ржирзНрждрж┐ ржмржЙ ржирж┐рж░рзНрж▓ржЬрзНржЬ рж╣рж╛рж╕рж┐ ржжрж┐ржпрж╝рзЗ ржЬржмрж╛ржм ржжрж┐рж▓рзЗржи, тАЬржЯрзБржЗрж╕рзНржЯ? ржЕржмрж╢рзНржпржЗ ржЖржЫрзЗтАФржХрж┐ржирзНрждрзБ рж╕рзЗржЯрж╛ рждрзЛ ржкрж░рзЗрж░ ржкрж░рзНржмрзЗрж░ ржмрзНржпрж╛ржкрж╛рж░!тАЭ рж╕рж╛ржмрж╛ржбрж╝рзЗ ржмрж╛ржЬрж╛рж░рзЗрж░ рж╕ржм рж╣ржЯрзНржЯржЧрзЛрж▓, ржорж╛ржЫрзЗрж░ рж░рж╛ржЬржХрзБржорж╛рж░ ржЖржмрж┐рж░рзНржнрж╛ржм, рж╢рзВржХрж░рзЗрж░ ржжрзМржбрж╝ржЭрж╛ржБржк, рж╢рж╛ржорзБржХрзЗрж░ ржлрзНрж▓рзНржпрж╛ржорж┐ржВ рж╢рзЛ, рж╕ржм ржорж┐рж▓рж┐ржпрж╝рзЗ ржпрзЗржи ржПржХ ржмрзБржБржж ржХрж╛рж░ржЦрж╛ржирж╛ред ржЖрж░ ржмрж░рзЗржи рждржЦржи ржмрзБржЭрж▓, ржЖржЬржХрзЗрж░ ржмрж╛ржЬрж╛рж░ ржПрж▓рзЗ рж╢рзБржзрзБ ржкрзБржБржЯрж┐ ржорж╛ржЫ ржиржпрж╝, ржЬрзАржмржирзЗрж░ рж╕ржарж┐ржХ ржкрж░рзНржмржУ ржорж┐рж╕ ржХрж░рж╛ ржпрж╛ржмрзЗ ржирж╛!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "label:\n",
      "label\n",
      "human    9057\n",
      "ai       9057\n",
      "Name: count, dtype: int64\n",
      "\n",
      "category:\n",
      "category\n",
      "novels     5000\n",
      "essays     5000\n",
      "stories    5000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('Human_AI.csv')  # or pd.read_excel(), pd.read_json()\n",
    "\n",
    "# Basic info\n",
    "print(\"ЁЯУД Basic Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nЁЯФв First 5 Rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nЁЯзн Dataset Shape:\")\n",
    "print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nЁЯХ│я╕П Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nPercentage of Missing Values:\")\n",
    "print((df.isnull().mean() * 100).round(2))\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"\\nЁЯзм Duplicated Rows:\")\n",
    "print(df.duplicated().sum())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nЁЯУж Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for constant columns\n",
    "print(\"\\nЁЯз▒ Constant Columns:\")\n",
    "print([col for col in df.columns if df[col].nunique() == 1])\n",
    "\n",
    "# Unique value counts\n",
    "print(\"\\nЁЯФН Unique Value Count per Column:\")\n",
    "print(df.nunique())\n",
    "\n",
    "# Describe numeric columns\n",
    "print(\"\\nЁЯУК Statistical Summary (Numeric):\")\n",
    "print(df.describe())\n",
    "\n",
    "# Describe categorical columns\n",
    "print(\"\\nЁЯЧВя╕П Summary (Categorical):\")\n",
    "print(df.describe(include='object'))\n",
    "\n",
    "# Value counts for categorical columns (top 3 categories)\n",
    "print(\"\\nЁЯз╛ Top Value Counts (Categorical Columns):\")\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df[col].value_counts().head(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e66331b-af2d-42af-b64f-75ef1fcae429",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1351dd79-07ef-4401-bbca-2fd86bb36a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72c652fa2804d629c982812d70c6ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14491 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712b23524e054bdabfa8d933e0c6600b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3623 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/g1/2lrwn79x3sd_lxd88z8hyfyc0000gn/T/ipykernel_42474/1257066206.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='5436' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   4/5436 07:09 < 324:20:24, 0.00 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 76\u001b[0m\n\u001b[1;32m     65\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     66\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     67\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     73\u001b[0m )\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n\u001b[1;32m     79\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./banglabert-human-ai\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2241\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2242\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   2243\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   2244\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2245\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:2555\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2548\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2549\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2553\u001b[0m )\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2555\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2558\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2561\u001b[0m ):\n\u001b[1;32m   2562\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2563\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:3745\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3744\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3745\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs, num_items_in_batch\u001b[38;5;241m=\u001b[39mnum_items_in_batch)\n\u001b[1;32m   3747\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3749\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3750\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3751\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:3810\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3808\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3809\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3810\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1503\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1503\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(\n\u001b[1;32m   1504\u001b[0m     input_ids,\n\u001b[1;32m   1505\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1506\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1507\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1508\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1509\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1510\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1511\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1512\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1513\u001b[0m )\n\u001b[1;32m   1515\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1517\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1016\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1016\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1017\u001b[0m     embedding_output,\n\u001b[1;32m   1018\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   1019\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1020\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1021\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m   1022\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1023\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1024\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1025\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1026\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1027\u001b[0m )\n\u001b[1;32m   1028\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1029\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:662\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    651\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    652\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    653\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    659\u001b[0m         output_attentions,\n\u001b[1;32m    660\u001b[0m     )\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 662\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    663\u001b[0m         hidden_states,\n\u001b[1;32m    664\u001b[0m         attention_mask,\n\u001b[1;32m    665\u001b[0m         layer_head_mask,\n\u001b[1;32m    666\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    667\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    668\u001b[0m         past_key_value,\n\u001b[1;32m    669\u001b[0m         output_attentions,\n\u001b[1;32m    670\u001b[0m     )\n\u001b[1;32m    672\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    542\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    549\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    551\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 552\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[1;32m    553\u001b[0m         hidden_states,\n\u001b[1;32m    554\u001b[0m         attention_mask,\n\u001b[1;32m    555\u001b[0m         head_mask,\n\u001b[1;32m    556\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    557\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[1;32m    558\u001b[0m     )\n\u001b[1;32m    559\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:482\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    474\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    481\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 482\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    483\u001b[0m         hidden_states,\n\u001b[1;32m    484\u001b[0m         attention_mask,\n\u001b[1;32m    485\u001b[0m         head_mask,\n\u001b[1;32m    486\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    487\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    488\u001b[0m         past_key_value,\n\u001b[1;32m    489\u001b[0m         output_attentions,\n\u001b[1;32m    490\u001b[0m     )\n\u001b[1;32m    491\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    492\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:407\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    405\u001b[0m )\n\u001b[0;32m--> 407\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[1;32m    408\u001b[0m     query_layer,\n\u001b[1;32m    409\u001b[0m     key_layer,\n\u001b[1;32m    410\u001b[0m     value_layer,\n\u001b[1;32m    411\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    412\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_prob \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    413\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m    414\u001b[0m )\n\u001b[1;32m    416\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    417\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Load and prepare dataset\n",
    "df = pd.read_csv(\"Human_AI.csv\")\n",
    "df = df[['text', 'label']]\n",
    "df = df[df['label'].isin(['human', 'ai'])]  # Filter out unexpected values\n",
    "df['label'] = df['label'].map({'human': 0, 'ai': 1})  # Encode labels\n",
    "\n",
    "# Split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "# Convert to HuggingFace Datasets\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"sagorsarker/bangla-bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "train_ds = train_ds.map(tokenize_function, batched=True)\n",
    "test_ds = test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds)\n",
    "    }\n",
    "\n",
    "# TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./banglabert-human-ai\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "model.save_pretrained(\"./banglabert-human-ai\")\n",
    "tokenizer.save_pretrained(\"./banglabert-human-ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccbefa2-2cbf-4a52-b796-4a1d9eb32f63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
