{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9441a15f-a0d6-4b97-b3af-f6f76deeb657",
   "metadata": {},
   "source": [
    "# Model Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e927b1b-1de5-4ddb-ab2a-4fb5e62ba562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (4.66.5)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: tokenizers in /opt/anaconda3/lib/python3.12/site-packages (0.21.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.33.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch datasets pandas numpy tqdm scikit-learn tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5445b864-6c87-4e48-ac08-33f12aa31335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39b9a79d-156d-4524-ac4d-349ee2a87ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable tokenizer parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a8e955e-7cb8-4240-b9aa-98a990cac691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7903d23d-9326-4c10-9cf6-17d2374918b1",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92d76cfb-308e-4c43-b36e-5c27adb81fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts: {0: 9500, 1: 9500}\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"cholito_bengali_human_ai_dataset.csv\")\n",
    "label_map = {\"human\": 0, \"ai\": 1}\n",
    "df[\"label\"] = df[\"label\"].map(label_map)\n",
    "texts = df[\"text\"].tolist()\n",
    "labels = df[\"label\"].tolist()\n",
    "print(f\"Label counts: {pd.Series(labels).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c28e29b-6df1-4ff4-b342-219c4b1c1875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label counts: {1: 7600, 0: 7600}\n",
      "Validation label counts: {1: 950, 0: 950}\n",
      "Test label counts: {1: 950, 0: 950}\n"
     ]
    }
   ],
   "source": [
    "# Data Splitting\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels, test_size=0.5, stratify=temp_labels, random_state=42\n",
    ")\n",
    "print(f\"Train label counts: {pd.Series(train_labels).value_counts().to_dict()}\")\n",
    "print(f\"Validation label counts: {pd.Series(val_labels).value_counts().to_dict()}\")\n",
    "print(f\"Test label counts: {pd.Series(test_labels).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a9d35-acfd-4b47-b955-d2cee6ac2b9e",
   "metadata": {},
   "source": [
    "# Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1759ec2f-d771-43e4-b9a4-a161f893e446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = WordPieceTrainer(vocab_size=30000, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\"])\n",
    "tokenizer.train_from_iterator(train_texts, trainer)\n",
    "tokenizer.save(\"bengali_wordpiece_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65a5d12a-4698-4aa6-9318-8056f7cd184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "def tokenize_texts(texts, tokenizer, max_length=128):\n",
    "    encodings = tokenizer.encode_batch(texts)\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for enc in encodings:\n",
    "        ids = enc.ids[:max_length] if len(enc.ids) > max_length else enc.ids + [0] * (max_length - len(enc.ids))\n",
    "        mask = [1] * len(enc.ids[:max_length]) + [0] * (max_length - len(enc.ids[:max_length]))\n",
    "        input_ids.append(ids)\n",
    "        attention_masks.append(mask)\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(attention_masks, dtype=torch.long)\n",
    "    }\n",
    "train_encodings = tokenize_texts(train_texts, tokenizer)\n",
    "val_encodings = tokenize_texts(val_texts, tokenizer)\n",
    "test_encodings = tokenize_texts(test_texts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01a1a173-67fc-4fc9-bdfc-6418f10e159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class BengaliTextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset = BengaliTextDataset(train_encodings, train_labels)\n",
    "val_dataset = BengaliTextDataset(val_encodings, val_labels)\n",
    "test_dataset = BengaliTextDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a63dacb1-28aa-4cf6-a7d7-7c4ba726efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff3c558-0bf0-466e-92f1-0ab8159774a7",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a801d5b-d372-4698-a7b4-3b6b8ddd7b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class CustomTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=8, num_encoder_layers=4, dim_feedforward=512, num_classes=2):\n",
    "        super(CustomTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embedding(input_ids) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoder(x)\n",
    "        if attention_mask is not None:\n",
    "            # Convert attention mask: 1 (valid token) -> False, 0 (padding) -> True\n",
    "            key_padding_mask = (attention_mask == 0)\n",
    "            x = self.transformer_encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        else:\n",
    "            x = self.transformer_encoder(x)\n",
    "        x = x[:, 0, :]  # Use [CLS] token representation\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize model\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "model = CustomTransformer(vocab_size=vocab_size)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "num_epochs = 10\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab9706d-5829-4e26-bc11-f9bde85685cb",
   "metadata": {},
   "source": [
    "# Set up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5c86f2f-9209-406b-b19c-d5b9b3035761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation functions\n",
    "def train_epoch(model, data_loader, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\", unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, device, desc=\"Evaluating\"):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    progress_bar = tqdm(data_loader, desc=desc, unit=\"batch\")\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    true_labels = np.array(true_labels)\n",
    "    accuracy = np.mean(predictions == true_labels)\n",
    "    cm = np.zeros((2, 2), dtype=int)\n",
    "    for t, p in zip(true_labels, predictions):\n",
    "        cm[t, p] += 1\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / len(data_loader),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"confusion_matrix\": cm.tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bad6bd-4a4a-4b5c-b422-61c69f310362",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e33e4a81-4a9f-4ffa-96b6-a55d51c2e95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████| 950/950 [06:05<00:00,  2.60batch/s, loss=0.73]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n",
      "Validating: 100%|██████████████| 119/119 [00:09<00:00, 11.96batch/s, loss=0.909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6927\n",
      "Validation Metrics: Loss: 0.6939, Accuracy: 0.5563, Precision: 0.5554, Recall: 0.5642, F1: 0.5598\n",
      "Confusion Matrix:\n",
      "[[521 429]\n",
      " [414 536]]\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████| 950/950 [05:46<00:00,  2.74batch/s, loss=0.726]\n",
      "Validating: 100%|██████████████| 119/119 [00:10<00:00, 11.17batch/s, loss=0.909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6910\n",
      "Validation Metrics: Loss: 0.6939, Accuracy: 0.5563, Precision: 0.5554, Recall: 0.5642, F1: 0.5598\n",
      "Confusion Matrix:\n",
      "[[521 429]\n",
      " [414 536]]\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████| 950/950 [05:58<00:00,  2.65batch/s, loss=0.866]\n",
      "Validating: 100%|██████████████| 119/119 [00:10<00:00, 11.29batch/s, loss=0.909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6917\n",
      "Validation Metrics: Loss: 0.6939, Accuracy: 0.5563, Precision: 0.5554, Recall: 0.5642, F1: 0.5598\n",
      "Confusion Matrix:\n",
      "[[521 429]\n",
      " [414 536]]\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████| 950/950 [05:34<00:00,  2.84batch/s, loss=0.755]\n",
      "Validating: 100%|██████████████| 119/119 [00:05<00:00, 20.06batch/s, loss=0.909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6939\n",
      "Validation Metrics: Loss: 0.6939, Accuracy: 0.5563, Precision: 0.5554, Recall: 0.5642, F1: 0.5598\n",
      "Confusion Matrix:\n",
      "[[521 429]\n",
      " [414 536]]\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████| 950/950 [03:26<00:00,  4.60batch/s, loss=0.741]\n",
      "Validating: 100%|██████████████| 119/119 [00:06<00:00, 17.41batch/s, loss=0.909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6937\n",
      "Validation Metrics: Loss: 0.6939, Accuracy: 0.5563, Precision: 0.5554, Recall: 0.5642, F1: 0.5598\n",
      "Confusion Matrix:\n",
      "[[521 429]\n",
      " [414 536]]\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████| 950/950 [03:30<00:00,  4.52batch/s, loss=0.825]\n",
      "Validating: 100%|██████████████| 119/119 [00:06<00:00, 18.17batch/s, loss=0.909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6944\n",
      "Validation Metrics: Loss: 0.6939, Accuracy: 0.5563, Precision: 0.5554, Recall: 0.5642, F1: 0.5598\n",
      "Confusion Matrix:\n",
      "[[521 429]\n",
      " [414 536]]\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████| 950/950 [03:34<00:00,  4.44batch/s, loss=0.714]\n",
      "Validating: 100%|██████████████| 119/119 [00:06<00:00, 18.08batch/s, loss=0.909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6925\n",
      "Validation Metrics: Loss: 0.6939, Accuracy: 0.5563, Precision: 0.5554, Recall: 0.5642, F1: 0.5598\n",
      "Confusion Matrix:\n",
      "[[521 429]\n",
      " [414 536]]\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████| 950/950 [03:34<00:00,  4.42batch/s, loss=0.728]\n",
      "Validating: 100%|██████████████| 119/119 [00:06<00:00, 18.18batch/s, loss=0.909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6940\n",
      "Validation Metrics: Loss: 0.6939, Accuracy: 0.5563, Precision: 0.5554, Recall: 0.5642, F1: 0.5598\n",
      "Confusion Matrix:\n",
      "[[521 429]\n",
      " [414 536]]\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████| 950/950 [03:32<00:00,  4.48batch/s, loss=0.688]\n",
      "Validating: 100%|██████████████| 119/119 [00:06<00:00, 19.22batch/s, loss=0.909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6929\n",
      "Validation Metrics: Loss: 0.6939, Accuracy: 0.5563, Precision: 0.5554, Recall: 0.5642, F1: 0.5598\n",
      "Confusion Matrix:\n",
      "[[521 429]\n",
      " [414 536]]\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████| 950/950 [03:27<00:00,  4.57batch/s, loss=0.721]\n",
      "Validating: 100%|██████████████| 119/119 [00:07<00:00, 15.33batch/s, loss=0.909]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6937\n",
      "Validation Metrics: Loss: 0.6939, Accuracy: 0.5563, Precision: 0.5554, Recall: 0.5642, F1: 0.5598\n",
      "Confusion Matrix:\n",
      "[[521 429]\n",
      " [414 536]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device, scheduler)\n",
    "    val_metrics = evaluate(model, val_loader, device, desc=\"Validating\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation Metrics: Loss: {val_metrics['loss']:.4f}, Accuracy: {val_metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{np.array(val_metrics['confusion_matrix'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a37d9a7e-6e7b-4db6-b705-77878e381a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████| 119/119 [00:07<00:00, 16.00batch/s, loss=0.611]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Metrics: Loss: 0.7027, Accuracy: 0.5479, Precision: 0.5462, Recall: 0.5663, F1: 0.5561\n",
      "Confusion Matrix:\n",
      "[[503 447]\n",
      " [412 538]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test evaluation\n",
    "test_metrics = evaluate(model, test_loader, device, desc=\"Testing\")\n",
    "print(f\"\\nTest Metrics: Loss: {test_metrics['loss']:.4f}, Accuracy: {test_metrics['accuracy']:.4f}, \"\n",
    "      f\"Precision: {test_metrics['precision']:.4f}, Recall: {test_metrics['recall']:.4f}, F1: {test_metrics['f1']:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{np.array(test_metrics['confusion_matrix'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b35259b-9ff8-4930-9be8-bbd55efef193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to 'bengali_custom_transformer.pt' and tokenizer saved to 'bengali_wordpiece_tokenizer.json'\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"bengali_custom_transformer.pt\")\n",
    "tokenizer.save(\"bengali_wordpiece_tokenizer.json\")\n",
    "print(\"Model saved to 'bengali_custom_transformer.pt' and tokenizer saved to 'bengali_wordpiece_tokenizer.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f5ee15-1635-4d57-b236-43a2378e5757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c961a96-eb74-457c-9353-17fe67a17214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97f211b2-4467-46f9-9b7f-b1a518701a3f",
   "metadata": {},
   "source": [
    "# Another One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "964ab124-4a47-41e2-8a70-686177299b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b3f091e-2b99-4986-a948-931c9db568cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "batch_size = 32\n",
    "max_length = 128\n",
    "num_epochs = 20\n",
    "learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "743809cf-c897-4b07-930f-2683e33479fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cholito_bengali_human_ai_dataset.csv\")\n",
    "df = df[df['label'].isin(['human', 'ai'])]  # Filter unwanted labels\n",
    "label_map = {\"human\": 0, \"ai\": 1}\n",
    "df[\"label\"] = df[\"label\"].map(label_map)\n",
    "texts, labels = df[\"text\"].tolist(), df[\"label\"].tolist()\n",
    "\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(texts, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, stratify=temp_labels, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb96160-bbca-4fff-9ff1-b37e201b0910",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c110e82f-0b4b-416c-904a-8e095a9568ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"bengali_tokenizer\"):\n",
    "    os.makedirs(\"bengali_tokenizer\")  # Ensure the directory exists\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "    tokenizer.train_from_iterator(train_texts, vocab_size=30000, min_frequency=2, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\"])\n",
    "    tokenizer.save_model(\"bengali_tokenizer\")\n",
    "else:\n",
    "    tokenizer = ByteLevelBPETokenizer(\"bengali_tokenizer/vocab.json\", \"bengali_tokenizer/merges.txt\")\n",
    "\n",
    "def encode_batch(texts):\n",
    "    encodings = tokenizer.encode_batch(texts)\n",
    "    input_ids, attention_masks = [], []\n",
    "    for e in encodings:\n",
    "        ids = e.ids[:max_length] + [0] * (max_length - len(e.ids)) if len(e.ids) < max_length else e.ids[:max_length]\n",
    "        mask = [1] * min(len(e.ids), max_length) + [0] * (max_length - len(e.ids))\n",
    "        input_ids.append(ids)\n",
    "        attention_masks.append(mask)\n",
    "    return torch.tensor(input_ids), torch.tensor(attention_masks)\n",
    "\n",
    "class BengaliTextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.input_ids, self.attention_mask = encode_batch(texts)\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = BengaliTextDataset(train_texts, train_labels)\n",
    "val_dataset = BengaliTextDataset(val_texts, val_labels)\n",
    "test_dataset = BengaliTextDataset(test_texts, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e19eaf4-f0f1-4f66-ab22-a058cc6d4a5e",
   "metadata": {},
   "source": [
    "# Custom Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d55bb90-62f8-4ce3-ace6-70581202eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len=128, d_model=512, nhead=8, num_layers=6, dim_feedforward=2048, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_len, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout=0.3, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        positions = torch.arange(0, input_ids.size(1), device=input_ids.device).unsqueeze(0)\n",
    "        x = self.embedding(input_ids) + self.position_embedding(positions)\n",
    "        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n",
    "        x = self.dropout(x[:, 0])  # Take CLS token representation\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa0f24b-efc8-44ce-8258-0ccc7af5df44",
   "metadata": {},
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c726c0a5-8421-4766-bfd8-161dee1fb754",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "model = CustomTransformer(vocab_size=vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Scheduler (optional warmup + decay)\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "warmup_steps = 500\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "830bfe40-b38a-476c-af4e-f3712518c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "    return total_loss / len(loader), accuracy, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec91239-779f-41ad-aa1a-9b0cb6ab8252",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d40f9cc-db01-47ff-bf16-217478166a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 475/475 [21:44<00:00,  2.75s/it]\n",
      "Evaluating: 100%|███████████████████████████████| 60/60 [00:55<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3401, Val Loss: 0.3786, Val Acc: 0.9274\n",
      "Saved new best model.\n",
      "\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 475/475 [22:21<00:00,  2.82s/it]\n",
      "Evaluating: 100%|███████████████████████████████| 60/60 [00:27<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3254, Val Loss: 0.3987, Val Acc: 0.8732\n",
      "\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 475/475 [20:36<00:00,  2.60s/it]\n",
      "Evaluating: 100%|███████████████████████████████| 60/60 [00:26<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5889, Val Loss: 0.6932, Val Acc: 0.5000\n",
      "\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 475/475 [15:36<00:00,  1.97s/it]\n",
      "Evaluating: 100%|███████████████████████████████| 60/60 [00:28<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6940, Val Loss: 0.6931, Val Acc: 0.5000\n",
      "\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|███                             | 45/475 [01:26<14:49,  2.07s/it]"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    train_loss = train_epoch()\n",
    "    val_loss, val_acc, _, _ = evaluate(val_loader)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save model if it improves\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_bengali_transformer.pt\")\n",
    "        print(\"Saved new best model.\")\n",
    "\n",
    "# Test\n",
    "print(\"\\nFinal Evaluation on Test Set:\")\n",
    "test_loss, test_acc, test_preds, test_labels = evaluate(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d86588-2897-46ab-88e7-91b3adbebfcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
